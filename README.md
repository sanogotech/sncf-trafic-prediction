# ğŸš† sncf-trafic-prediction

PrÃ©diction des retards des trains SNCF Ã  partir de donnÃ©es de passages en gare, en temps rÃ©el ou batch, via une stack Big Data moderne (Spark, Kafka, Delta, Airflow, Power BI).

## ğŸ¯ Objectif

Ce projet vise Ã  **prÃ©voir les retards des trains** Ã  partir de donnÃ©es de passages en gare, en s'appuyant sur une architecture de **traitement temps rÃ©el** et un pipeline complet de **Machine Learning**, depuis l'ingestion des donnÃ©es jusqu'Ã  la visualisation.

## ğŸ§± Stack technique

| Composant                 | RÃ´le                                                                 |
|---------------------------|----------------------------------------------------------------------|
| **Docker + Compose**      | Conteneurisation et orchestration des services                       |
| **Apache Kafka**          | Ingestion en streaming des donnÃ©es simulÃ©es                          |
| **Apache Spark**          | Traitement des donnÃ©es + entraÃ®nement ML avec Spark MLlib            |
| **Delta Lake**            | Format de stockage transactionnel sur Spark                          |
| **Airflow**               | Orchestration des tÃ¢ches ETL + ML                                    |
| **PostgreSQL**            | Backend pour Airflow                                                 |
| **Power BI (DirectQuery)**| Visualisation temps rÃ©el (optionnelle)                               |
| **Python + Pandas + scikit-learn** | PrÃ©paration des donnÃ©es et export vers Power BI             |

## âš™ï¸ Architecture du projet

```
.
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ jupyter/              # Dockerfile custom Spark + Delta + Jupyter
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_model.py        # EntraÃ®nement ML
â”‚   â””â”€â”€ push_prediction_to_powerbi.py  # Export vers Power BI (API REST)
â”œâ”€â”€ data/                     # DonnÃ©es brutes (exclues du dÃ©pÃ´t GitHub)
â”œâ”€â”€ notebooks/                # Analyses exploratoires
â”œâ”€â”€ dags/                     # DAGs Airflow (exclus de ce dÃ©pÃ´t)
â”œâ”€â”€ docker-compose.yml        # Stack complÃ¨te
â””â”€â”€ README.md
```

## ğŸ” Pipeline complet

1. **Kafka simule** des messages de trafic ferroviaire.
2. **Airflow** dÃ©clenche `train_model.py` (batch ML en Spark).
3. Les prÃ©dictions sont stockÃ©es et/ou envoyÃ©es Ã  Power BI.
4. `push_prediction_to_powerbi.py` publie les donnÃ©es prÃªtes Ã  visualiser.

## âœ… Avancement

| Ã‰tape                                 | Statut  |
|---------------------------------------|---------|
| Environnement Docker complet          | âœ… Fait |
| Kafka + Zookeeper                     | âœ… Fait |
| PostgreSQL (Airflow)                  | âœ… Fait |
| Spark avec Delta Lake                 | âœ… Fait |
| Script d'entraÃ®nement ML (Spark)      | âœ… Fait |
| Orchestration avec Airflow            | âœ… Fait |
| Export des prÃ©dictions (Power BI)     | âœ… Fait |
| Dashboard Power BI                    | âŒ Optionnel (non finalisÃ©) |

## ğŸ§  Points forts techniques

- Utilisation de **Delta Lake** pour les performances et lâ€™atomicitÃ©.
- Traitement **distribuÃ© et scalable** avec Spark.
- Architecture **modulaire**, conteneurisÃ©e, et automatisÃ©e.
- Support de **traitement batch + streaming**.
- PossibilitÃ© dâ€™extension facile vers du temps rÃ©el complet avec Kafka + Spark Structured Streaming.

## ğŸ“¦ GÃ©nÃ©ration locale des donnÃ©es

CrÃ©ez le producteur Kafka et lancez l'enrichissement en local :

```bash
bash bootstrap/produce_data.sh
```

Ce script :
- Lance le simulateur Kafka
- DÃ©clenche le traitement Spark
- Ã‰crit un fichier enrichi exportable vers Power BI

## ğŸš€ Lancer le projet

```bash
# DÃ©marrer tous les services
docker-compose up -d --build

# AccÃ©der Ã  Jupyter : http://localhost:8888
# AccÃ©der Ã  Airflow : http://localhost:8080
```

## ğŸ“Š (Facultatif) Mesures Power BI

- `Nombre Trajets` : `COUNT(RealTimeData[trip_id])`
- `Retard Moyen (s)` : `AVERAGE(RealTimeData[departure] - RealTimeData[arrival])`
- `DurÃ©e Moyenne Parcours (s)` : `AVERAGE(RealTimeData[arrival] - RealTimeData[departure])`
ğŸ“š RÃ©fÃ©rences & Sources de donnÃ©es
Ce projet sâ€™appuie sur des donnÃ©es publiques et ouvertes, provenant de plateformes fiables :

ğŸ”¹ DonnÃ©es SNCF officielles
Horaires des lignes TER SNCF (GTFS & temps rÃ©el)
â€¢ transport.data.gouv.fr (dataset principal)
https://transport.data.gouv.fr/datasets/horaires-des-lignes-ter-sncf
â€¢ data.gouv.fr (miroir officiel)
https://www.data.gouv.fr/fr/datasets/horaires-des-lignes-ter-sncf/

Ces jeux de donnÃ©es incluent :

Les fichiers GTFS statiques : stops.txt, trips.txt, stop_times.txt, etc.

Des mises Ã  jour en temps rÃ©el de type GTFS-RT (trip updates, vehicle positions)

ğŸ”¹ Localisation des gares
Liste complÃ¨te des gares franÃ§aises (avec coordonnÃ©es gÃ©ographiques)
â€¢ Kaggle - Dataset de Nathan 
https://www.kaggle.com/datasets/nathanlauga/french-train-station?select=liste-des-gares.csv
## ğŸ™‹ Ã€ propos

**Auteur : Aldiouma Mbaye**  
Projet personnel rÃ©alisÃ© pour approfondir lâ€™architecture Big Data & MLOps avec Kafka, Spark et Airflow.  

